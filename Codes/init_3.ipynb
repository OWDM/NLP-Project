{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Musae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Musae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Arabic and English punctuation\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "# Define Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Function to normalize Arabic script\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)  # Corrected this line\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to process and stem Arabic text using ISRI stemmer\n",
    "def process_text(text):\n",
    "    # Normalize Arabic text\n",
    "    text = normalize_arabic(text)\n",
    "    # Tokenize text\n",
    "    word_list = word_tokenize(text)\n",
    "    # Initialize the ISRI stemmer\n",
    "    stemmer = ISRIStemmer()\n",
    "    # Apply stemming to each word\n",
    "    word_list = [stemmer.stem(w) for w in word_list if w not in punctuations_list and w not in arabic_stopwords]\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
      "1  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
      "2  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
      "3  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
      "4  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
      "5  أسلوب الكاتب رائع جدا و عميق جدا، قرأته عدة مر...   \n",
      "6  استثنائي. الهدوء في الجناح مع مسبح. عدم وجود ع...   \n",
      "7  الكتاب هو السيرة الذاتية للحداثة في المملكة بل...   \n",
      "8       من أجمل ما قرأت.. رواية تستحق القراءة فعلا..   \n",
      "9  بشكل عام جيده .. . التجاوب جيد جدا من قبل موظف...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                      متز نوع نظف وقع جهز شاطيء طعم  \n",
      "1  احد سبب نجح امر ان شخص دول عشق ترب نحب امر ومض...  \n",
      "2  هدف .. وقي نقل صخب شرع قهر الي هدء جبل شيش .. ...  \n",
      "3  خلص .. بدي الل مست بهر زي فيل زرق ميقراش احس ....  \n",
      "4       ياس جلر جزء تجز دبي ندق كامل خدم ريح نفس وجد  \n",
      "5  سلب كتب ريع جدا عمق جد، قرت عده رات كنت طلب رح...  \n",
      "6       استثنايي هدء جنح سبح عدم وجد عزل جيد غرف عدي  \n",
      "7  كتب سير ذتي حدث ملك بلس برز معاصريها، حلل جمع ...  \n",
      "8                      جمل قرت .. ريه سحق قرء فعل ..  \n",
      "9  شكل عام جيد .. جوب جيد جدا وظف قبل خدم وصل حرم...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\NLP-Project\\\\data\\\\ar_reviews_100k.tsv', sep='\\t')\n",
    "\n",
    "# Apply the new processing function to the text column\n",
    "df['cleaned_text'] = df['text'].apply(process_text)\n",
    "\n",
    "# Display the original and processed text for the first 10 entries\n",
    "print(df[['text', 'cleaned_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_keywords = {\n",
    "    'ممتاز', 'جيد', 'رائع', 'سعيد', 'لذيذ', 'مبهج', 'فرح', 'استثنائي', 'جميل', 'محبب', 'ممتع',\n",
    "    'مذهل', 'مريح', 'راض', 'أحب', 'استمتع', 'مفاجأة', 'مميز', 'لطيف', 'مرح', 'معجزة',\n",
    "    'ملهم', 'أسعد', 'خيالي', 'مذهل', 'فريد', 'هائل', 'راقي', 'أنيق', 'بهجة', 'مفيد',\n",
    "    'قيمة', 'بسيط', 'ناجح', 'موفق', 'مشجع', 'حسن', 'ظريف', 'محبوب', 'مبهر', 'إيجابي',\n",
    "    'تفاؤل', 'إعجاب', 'ممتن', 'شجاع', 'آمن', 'مثالي'\n",
    "}\n",
    "\n",
    "negative_keywords = {\n",
    "    'سيء', 'مخيب', 'حزين', 'مؤلم', 'كريه', 'قبيح', 'فشل', 'محبط', 'بشع', 'فظيع', 'مزعج', \n",
    "    'مروع', 'أسوأ', 'كره', 'كارثة', 'رعب', 'كئيب', 'مزعزع', 'اكتئاب', 'بائس', 'معقد', \n",
    "    'إحباط', 'تعب', 'مضجر', 'ممل', 'فضيحة', 'سلبي', 'كاذب', 'فظاظة', 'احتيال', 'احراج',\n",
    "    'بشع', 'تعيس', 'مستاء', 'مروع', 'مشؤوم', 'عداء', 'مزري', 'عنيف', 'غير كاف', 'غير صادق',\n",
    "    'ضعيف', 'متشائم', 'غاضب', 'غير مقبول', 'مروع'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
      "1  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
      "2  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
      "3  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
      "4  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
      "5  أسلوب الكاتب رائع جدا و عميق جدا، قرأته عدة مر...   \n",
      "6  استثنائي. الهدوء في الجناح مع مسبح. عدم وجود ع...   \n",
      "7  الكتاب هو السيرة الذاتية للحداثة في المملكة بل...   \n",
      "8       من أجمل ما قرأت.. رواية تستحق القراءة فعلا..   \n",
      "9  بشكل عام جيده .. . التجاوب جيد جدا من قبل موظف...   \n",
      "\n",
      "                                        cleaned_text     label predicted_label  \n",
      "0                      متز نوع نظف وقع جهز شاطيء طعم  Positive           Mixed  \n",
      "1  احد سبب نجح امر ان شخص دول عشق ترب نحب امر ومض...  Positive           Mixed  \n",
      "2  هدف .. وقي نقل صخب شرع قهر الي هدء جبل شيش .. ...  Positive           Mixed  \n",
      "3  خلص .. بدي الل مست بهر زي فيل زرق ميقراش احس ....  Positive           Mixed  \n",
      "4       ياس جلر جزء تجز دبي ندق كامل خدم ريح نفس وجد  Positive           Mixed  \n",
      "5  سلب كتب ريع جدا عمق جد، قرت عده رات كنت طلب رح...  Positive           Mixed  \n",
      "6       استثنايي هدء جنح سبح عدم وجد عزل جيد غرف عدي  Positive        Positive  \n",
      "7  كتب سير ذتي حدث ملك بلس برز معاصريها، حلل جمع ...  Positive           Mixed  \n",
      "8                      جمل قرت .. ريه سحق قرء فعل ..  Positive           Mixed  \n",
      "9  شكل عام جيد .. جوب جيد جدا وظف قبل خدم وصل حرم...  Positive        Positive  \n",
      "\n",
      "Predicted sentiment counts:\n",
      "predicted_label\n",
      "Mixed       77226\n",
      "Positive    15533\n",
      "Negative     7240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rule-based classification function\n",
    "def classify_review(text):\n",
    "    # Tokenize the input text\n",
    "    words = set(text.split())\n",
    "    # Initialize flags to check for keyword presence\n",
    "    has_positive = bool(words & positive_keywords)\n",
    "    has_negative = bool(words & negative_keywords)\n",
    "\n",
    "    # Basic rule-based classification\n",
    "    if has_positive and not has_negative:\n",
    "        return \"Positive\"\n",
    "    elif has_negative and not has_positive:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Mixed\"\n",
    "\n",
    "# Apply classification to cleaned text\n",
    "df['predicted_label'] = df['cleaned_text'].apply(classify_review)\n",
    "\n",
    "# Display the original, cleaned, and predicted labels for the first 10 reviews\n",
    "print(df[['text', 'cleaned_text', 'label', 'predicted_label']].head(10))\n",
    "\n",
    "# Evaluate the classifier\n",
    "predicted_counts = df['predicted_label'].value_counts()\n",
    "\n",
    "print(\"\\nPredicted sentiment counts:\")\n",
    "print(predicted_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
