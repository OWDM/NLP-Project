{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Tashaphyne in c:\\users\\musae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: pyarabic in c:\\users\\musae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Tashaphyne) (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\musae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyarabic->Tashaphyne) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Tashaphyne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement isri (from versions: none)\n",
      "ERROR: No matching distribution found for isri\n"
     ]
    }
   ],
   "source": [
    "!pip install isri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Musae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Musae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "# Download NLTK resources if not already available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Arabic Light Stemmer\n",
    "ArListem = ArabicLightStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Arabic and English punctuation\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "# Normalization function for Arabic\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Preprocessing function to clean and stem text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize text\n",
    "    text = normalize_arabic(text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and stopwords, convert to lowercase\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word not in punctuations_list and word not in arabic_stopwords]\n",
    "    # Stem each token using Arabic Light Stemmer\n",
    "    stemmed_tokens = [ArListem.light_stem(word) or ArListem.get_root() for word in cleaned_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
      "1  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
      "2  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
      "3  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
      "4  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
      "5  أسلوب الكاتب رائع جدا و عميق جدا، قرأته عدة مر...   \n",
      "6  استثنائي. الهدوء في الجناح مع مسبح. عدم وجود ع...   \n",
      "7  الكتاب هو السيرة الذاتية للحداثة في المملكة بل...   \n",
      "8       من أجمل ما قرأت.. رواية تستحق القراءة فعلا..   \n",
      "9  بشكل عام جيده .. . التجاوب جيد جدا من قبل موظف...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0              ممتاز وعا نظافه موقع تجهيز شاطيء مطعم  \n",
      "1  حد سباب جاح امار ان شخص دوله يعشق راب حب امار ...  \n",
      "2  هادف .. قو نقل صخب شوارع قاهره لي هدوء جبال شي...  \n",
      "3  خلص .. مبديي لي مس بهار زي فيل ازرق ميقراش حس ...  \n",
      "4   ياس جلوري جزء تجز دب دق متكامل خدم مريح فسيا وجد  \n",
      "5  سلوب كاتب رايع جد عميق جدا، قر عد مر نت طالب م...  \n",
      "6  استثناي هدوء جناح مسبح عدم جود عازل جيد غرف عاديه  \n",
      "7  كتاب سيره ذاتيه حداثه مملكه لسان برز معاصريها،...  \n",
      "8                     جمل قر .. روا ستحق قراءه عل ..  \n",
      "9  شكل عام جيد .. تجاوب جيد جد موظف استقبال خدم ت...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\NLP-Project\\\\data\\\\ar_reviews_100k.tsv', sep='\\t')\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first 10 entries of the cleaned text\n",
    "print(df[['text', 'cleaned_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_keywords = {\n",
    "    'ممتاز', 'جيد', 'رائع', 'سعيد', 'لذيذ', 'مبهج', 'فرح', 'استثنائي', 'جميل', 'محبب', 'ممتع',\n",
    "    'مذهل', 'مريح', 'راض', 'أحب', 'استمتع', 'مفاجأة', 'مميز', 'لطيف', 'مرح', 'معجزة',\n",
    "    'ملهم', 'أسعد', 'خيالي', 'مذهل', 'فريد', 'هائل', 'راقي', 'أنيق', 'بهجة', 'مفيد',\n",
    "    'قيمة', 'بسيط', 'ناجح', 'موفق', 'مشجع', 'حسن', 'ظريف', 'محبوب', 'مبهر', 'إيجابي',\n",
    "    'تفاؤل', 'إعجاب', 'ممتن', 'شجاع', 'آمن', 'مثالي'\n",
    "}\n",
    "\n",
    "negative_keywords = {\n",
    "    'سيء', 'مخيب', 'حزين', 'مؤلم', 'كريه', 'قبيح', 'فشل', 'محبط', 'بشع', 'فظيع', 'مزعج', \n",
    "    'مروع', 'أسوأ', 'كره', 'كارثة', 'رعب', 'كئيب', 'مزعزع', 'اكتئاب', 'بائس', 'معقد', \n",
    "    'إحباط', 'تعب', 'مضجر', 'ممل', 'فضيحة', 'سلبي', 'كاذب', 'فظاظة', 'احتيال', 'احراج',\n",
    "    'بشع', 'تعيس', 'مستاء', 'مروع', 'مشؤوم', 'عداء', 'مزري', 'عنيف', 'غير كاف', 'غير صادق',\n",
    "    'ضعيف', 'متشائم', 'غاضب', 'غير مقبول', 'مروع'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
      "1  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
      "2  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
      "3  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
      "4  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
      "5  أسلوب الكاتب رائع جدا و عميق جدا، قرأته عدة مر...   \n",
      "6  استثنائي. الهدوء في الجناح مع مسبح. عدم وجود ع...   \n",
      "7  الكتاب هو السيرة الذاتية للحداثة في المملكة بل...   \n",
      "8       من أجمل ما قرأت.. رواية تستحق القراءة فعلا..   \n",
      "9  بشكل عام جيده .. . التجاوب جيد جدا من قبل موظف...   \n",
      "\n",
      "                                        cleaned_text     label predicted_label  \n",
      "0              ممتاز وعا نظافه موقع تجهيز شاطيء مطعم  Positive        Positive  \n",
      "1  حد سباب جاح امار ان شخص دوله يعشق راب حب امار ...  Positive           Mixed  \n",
      "2  هادف .. قو نقل صخب شوارع قاهره لي هدوء جبال شي...  Positive           Mixed  \n",
      "3  خلص .. مبديي لي مس بهار زي فيل ازرق ميقراش حس ...  Positive           Mixed  \n",
      "4   ياس جلوري جزء تجز دب دق متكامل خدم مريح فسيا وجد  Positive        Positive  \n",
      "5  سلوب كاتب رايع جد عميق جدا، قر عد مر نت طالب م...  Positive           Mixed  \n",
      "6  استثناي هدوء جناح مسبح عدم جود عازل جيد غرف عاديه  Positive        Positive  \n",
      "7  كتاب سيره ذاتيه حداثه مملكه لسان برز معاصريها،...  Positive        Positive  \n",
      "8                     جمل قر .. روا ستحق قراءه عل ..  Positive           Mixed  \n",
      "9  شكل عام جيد .. تجاوب جيد جد موظف استقبال خدم ت...  Positive        Positive  \n",
      "\n",
      "Predicted sentiment counts:\n",
      "predicted_label\n",
      "Mixed       58053\n",
      "Positive    30325\n",
      "Negative    11621\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rule-based classification function\n",
    "def classify_review(text):\n",
    "    # Tokenize the input text\n",
    "    words = set(text.split())\n",
    "    # Initialize flags to check for keyword presence\n",
    "    has_positive = bool(words & positive_keywords)\n",
    "    has_negative = bool(words & negative_keywords)\n",
    "\n",
    "    # Basic rule-based classification\n",
    "    if has_positive and not has_negative:\n",
    "        return \"Positive\"\n",
    "    elif has_negative and not has_positive:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Mixed\"\n",
    "\n",
    "# Apply classification to cleaned text\n",
    "df['predicted_label'] = df['cleaned_text'].apply(classify_review)\n",
    "\n",
    "# Display the original, cleaned, and predicted labels for the first 10 reviews\n",
    "print(df[['text', 'cleaned_text', 'label', 'predicted_label']].head(10))\n",
    "\n",
    "# Evaluate the classifier\n",
    "predicted_counts = df['predicted_label'].value_counts()\n",
    "\n",
    "print(\"\\nPredicted sentiment counts:\")\n",
    "print(predicted_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
